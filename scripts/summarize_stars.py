import os
import time
import requests
import json
import concurrent.futures
from typing import Dict, List, Optional

# é…ç½®å¸¸é‡
GITHUB_USERNAME = "WuXiangM"
GITHUB_TOKEN = os.environ.get("STARRED_GITHUB_TOKEN")
OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY")

# API é…ç½®
DEFAULT_COPILOT_MODEL = "openai/gpt-4o-mini"
DEFAULT_OPENROUTER_MODEL = "deepseek/deepseek-prover-v2:free"
MAX_WORKERS = 3  # é™ä½å¹¶å‘æ•°ä»¥é¿å… 429 é”™è¯¯
BATCH_SIZE = 5   # å‡å°æ‰¹æ¬¡å¤§å°
REQUEST_TIMEOUT = 60
RATE_LIMIT_DELAY = 10  # å¢åŠ å»¶è¿Ÿæ—¶é—´
REQUEST_RETRY_DELAY = 30  # é‡åˆ° 429 é”™è¯¯æ—¶çš„é‡è¯•å»¶è¿Ÿ

# è¾“å‡ºé…ç½®
README_SUM_PATH = "README-sum.md"

# æ‰“å° API Key å‰ç¼€ç”¨äºè°ƒè¯•
if OPENROUTER_API_KEY:
    print(f"OpenRouter API Key å‰ç¼€: {OPENROUTER_API_KEY[:8]}...")
if GITHUB_TOKEN:
    print(f"GitHub Token å‰ç¼€: {GITHUB_TOKEN[:8]}...")


def get_starred_repos() -> List[Dict]:
    """è·å–ç”¨æˆ·çš„ GitHub æ˜Ÿæ ‡ä»“åº“"""
    if not GITHUB_TOKEN:
        raise ValueError("ç¼ºå°‘ GITHUB_TOKEN ç¯å¢ƒå˜é‡")
    
    print("æ­£åœ¨è·å–æ˜Ÿæ ‡ä»“åº“...")
    repos = []
    page = 1
    per_page = 100
    headers = {
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Accept": "application/vnd.github+json"
    }
    
    while True:
        try:
            url = f"https://api.github.com/users/{GITHUB_USERNAME}/starred?per_page={per_page}&page={page}"
            resp = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            data = resp.json()
            
            if not data:
                break
                
            repos.extend(data)
            print(f"å·²è·å– {len(repos)} ä¸ªä»“åº“... (ç¬¬ {page} é¡µ)")
            page += 1
            
            # é¿å… GitHub API é™åˆ¶
            time.sleep(1)
            
        except requests.RequestException as e:
            print(f"è·å–æ˜Ÿæ ‡ä»“åº“å¤±è´¥: {e}")
            break
    
    print(f"æ€»å…±è·å–åˆ° {len(repos)} ä¸ªæ˜Ÿæ ‡ä»“åº“")
    return repos


def load_old_summaries():
    """è¯»å–æ—§çš„README-sum.mdï¼Œè¿”å›å­—å…¸: {repo_full_name: summary}"""
    if not os.path.exists(README_SUM_PATH):
        return {}
    summaries = {}
    current_repo = None
    current_lines = []
    with open(README_SUM_PATH, encoding="utf-8") as f:
        for line in f:
            if line.startswith("### ["):
                if current_repo and current_lines:
                    summaries[current_repo] = "".join(current_lines).strip()
                # è§£æä»“åº“å
                left = line.find('[') + 1
                right = line.find(']')
                current_repo = line[left:right]
                current_lines = []
            elif current_repo:
                current_lines.append(line)
        if current_repo and current_lines:
            summaries[current_repo] = "".join(current_lines).strip()
    return summaries


def openrouter_summarize(repo: Dict) -> Optional[str]:
    """ä½¿ç”¨ OpenRouter API æ€»ç»“ä»“åº“"""
    repo_name = repo["full_name"]
    desc = repo.get("description") or ""
    url = repo["html_url"]

    prompt = (
        f"è¯·å¯¹ä»¥ä¸‹ GitHub ä»“åº“è¿›è¡Œå†…å®¹æ€»ç»“ï¼ŒæŒ‰å¦‚ä¸‹æ ¼å¼è¾“å‡ºï¼ˆç”¨ä¸­æ–‡ï¼‰ï¼š\n"
        f"1. **ä»“åº“åç§°ï¼š** {repo_name}\n"
        f"2. **ç®€è¦ä»‹ç»ï¼š** ï¼ˆ50å­—ä»¥å†…ï¼‰\n"
        f"3. **åˆ›æ–°ç‚¹ï¼š** ï¼ˆç®€è¿°æœ¬ä»“åº“æœ€æœ‰ç‰¹è‰²çš„åœ°æ–¹ï¼‰\n"
        f"4. **ç®€å•ç”¨æ³•ï¼š** ï¼ˆç»™å‡ºæœ€ç®€å…³é”®ç”¨æ³•æˆ–è°ƒç”¨ç¤ºä¾‹ï¼Œå¦‚æ— åˆ™ç•¥ï¼‰\n"
        f"5. **æ€»ç»“ï¼š** ï¼ˆä¸€å¥è¯æ€»ç»“å®ƒçš„ç”¨é€”/ä»·å€¼ï¼‰\n"
        f"**ä»“åº“æè¿°ï¼š** {desc}\n"
        f"**ä»“åº“åœ°å€ï¼š** {url}\n"
    )

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": DEFAULT_OPENROUTER_MODEL,
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ]
    }

    try:
        response = requests.post(
            url="https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            data=json.dumps(data),
            timeout=REQUEST_TIMEOUT
        )
        if response.status_code == 429:
            raise requests.HTTPError("429 Too Many Requests")
        response.raise_for_status()
        content = response.json()['choices'][0]['message']['content'].strip()
        return content
    except Exception as e:
        print(f"OpenRouter API è°ƒç”¨å¤±è´¥: {e}")
        return None

# æ–°å¢ï¼šä½¿ç”¨ GitHub Copilot / GitHub Models API è¿›è¡Œæ€»ç»“
# éœ€è¦ STARRED_GITHUB_TOKEN å…·å¤‡è®¿é—® models:read & copilot èŒƒå›´ï¼ˆä¸€èˆ¬ PAT å¯ç”¨ copilot å³å¯ï¼‰
# å¯é€šè¿‡ç¯å¢ƒå˜é‡ GITHUB_COPILOT_MODEL æŒ‡å®šæ¨¡å‹ï¼Œé»˜è®¤ gpt-4o-miniï¼ˆä¾æ® GitHub Models å¯ç”¨æ¨¡å‹è‡ªè¡Œè°ƒæ•´ï¼‰
def copilot_summarize(repo: Dict) -> Optional[str]:
    """ä½¿ç”¨ GitHub Copilot / GitHub Models API è¿›è¡Œæ€»ç»“"""
    repo_name = repo["full_name"]
    desc = repo.get("description") or ""
    url = repo["html_url"]
    model = os.environ.get("GITHUB_COPILOT_MODEL", DEFAULT_COPILOT_MODEL)

    prompt = (
        f"è¯·å¯¹ä»¥ä¸‹ GitHub ä»“åº“è¿›è¡Œå†…å®¹æ€»ç»“ï¼ŒæŒ‰å¦‚ä¸‹æ ¼å¼è¾“å‡ºï¼ˆç”¨ä¸­æ–‡ï¼‰ï¼š\n"
        f"1. **ä»“åº“åç§°ï¼š** {repo_name}\n"
        f"2. **ç®€è¦ä»‹ç»ï¼š** ï¼ˆ50å­—ä»¥å†…ï¼‰\n"
        f"3. **åˆ›æ–°ç‚¹ï¼š** ï¼ˆç®€è¿°æœ¬ä»“åº“æœ€æœ‰ç‰¹è‰²çš„åœ°æ–¹ï¼‰\n"
        f"4. **ç®€å•ç”¨æ³•ï¼š** ï¼ˆç»™å‡ºæœ€ç®€å…³é”®ç”¨æ³•æˆ–è°ƒç”¨ç¤ºä¾‹ï¼Œå¦‚æ— åˆ™ç•¥ï¼‰\n"
        f"5. **æ€»ç»“ï¼š** ï¼ˆä¸€å¥è¯æ€»ç»“å®ƒçš„ç”¨é€”/ä»·å€¼ï¼‰\n"
        f"**ä»“åº“æè¿°ï¼š** {desc}\n"
        f"**ä»“åº“åœ°å€ï¼š** {url}\n"
    )

    if not GITHUB_TOKEN:
        print("ç¼ºå°‘ STARRED_GITHUB_TOKENï¼Œæ— æ³•è°ƒç”¨ GitHub Copilot API")
        return None

    headers = {
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Accept": "application/json",
        "X-GitHub-Api-Version": "2023-07-01",
        "Content-Type": "application/json"
    }

    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 600,
        "temperature": 0.4
    }

    max_retries = 3
    for attempt in range(max_retries):
        try:
            resp = requests.post(
                "https://models.github.ai/inference/chat/completions",
                headers=headers,
                data=json.dumps(data),
                timeout=REQUEST_TIMEOUT
            )
            if resp.status_code == 429:
                if attempt < max_retries - 1:
                    print(f"é‡åˆ° 429 é”™è¯¯ï¼Œç­‰å¾… {REQUEST_RETRY_DELAY} ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(REQUEST_RETRY_DELAY)
                    continue
                else:
                    raise requests.HTTPError("429 Too Many Requests")
            resp.raise_for_status()
            j = resp.json()
            content = j.get('choices', [{}])[0].get('message', {}).get('content', '')
            return content.strip() if content else None
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"API è°ƒç”¨å¤±è´¥ï¼Œç­‰å¾… {REQUEST_RETRY_DELAY} ç§’åé‡è¯•: {e}")
                time.sleep(REQUEST_RETRY_DELAY)
                continue
            else:
                print(f"GitHub Models API è°ƒç”¨å¤±è´¥: {e}")
                return None
    
    return None


def is_valid_summary(summary: str) -> bool:
    """æ£€æŸ¥ç»™å®šçš„æ€»ç»“æ˜¯å¦æœ‰æ•ˆï¼ˆä¸åŒ…å«ç”Ÿæˆå¤±è´¥ç­‰å†…å®¹ï¼‰"""
    invalid_phrases = ["ç”Ÿæˆå¤±è´¥", "æš‚æ— AIæ€»ç»“", "429"]
    return not any(phrase in summary for phrase in invalid_phrases)


def summarize_batch(repos: List[Dict], old_summaries: Dict[str, str], use_copilot: bool = False) -> List[str]:
    """æ‰¹é‡æ€»ç»“ä»“åº“ï¼Œæ”¯æŒé€‰æ‹©ä½¿ç”¨ OpenRouter æˆ– GitHub Copilot"""
    results = [None] * len(repos)
    summarize_func = copilot_summarize if use_copilot else openrouter_summarize
    api_name = "Copilot" if use_copilot else "OpenRouter"
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_idx = {
            executor.submit(summarize_func, repo): idx
            for idx, repo in enumerate(repos)
        }
        for future in concurrent.futures.as_completed(future_to_idx):
            idx = future_to_idx[future]
            repo = repos[idx]
            try:
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰æœ‰æ•ˆæ€»ç»“
                existing_summary = old_summaries.get(repo["full_name"], "")
                if is_valid_summary(existing_summary):
                    summary = existing_summary
                else:
                    summary = future.result()
                    if summary is None:  # 429ç­‰å¤±è´¥
                        summary = old_summaries.get(repo["full_name"], f"{api_name} APIç”Ÿæˆå¤±è´¥æˆ–429")
            except Exception as exc:
                print(f"{repo['full_name']} çº¿ç¨‹å¼‚å¸¸: {exc}")
                summary = old_summaries.get(repo["full_name"], f"{api_name} APIç”Ÿæˆå¤±è´¥")
            results[idx] = summary
    return results


def copilot_summarize_batch(repos: List[Dict], old_summaries: Dict[str, str]) -> List[str]:
    """ä½¿ç”¨ GitHub Copilot æ‰¹é‡æ€»ç»“ä»“åº“"""
    return summarize_batch(repos, old_summaries, use_copilot=True)


def classify_by_language(repos):
    classified = {}
    for repo in repos:
        lang = repo.get("language") or "Other"
        classified.setdefault(lang, []).append(repo)
    return classified

###########################################
def main():
    # é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ä½¿ç”¨å“ªç§ APIï¼Œé»˜è®¤ä½¿ç”¨ Copilot
    use_copilot_api = os.environ.get("USE_COPILOT_API", "true").lower() == "true"
    api_name = "GitHub Copilot" if use_copilot_api else "OpenRouter (DeepSeek)"
    
    print(f"å¼€å§‹ä½¿ç”¨ {api_name} ç”Ÿæˆ GitHub Star é¡¹ç›®æ€»ç»“...")
    
    try:
        starred = get_starred_repos()
        classified = classify_by_language(starred)
        old_summaries = load_old_summaries()
        
        # æ›´æ–°æ ‡é¢˜ä»¥åæ˜ å®é™…ä½¿ç”¨çš„ API
        current_time = time.strftime("%Yå¹´%mæœˆ%dæ—¥", time.localtime())
        title = f"# æˆ‘çš„ GitHub Star é¡¹ç›®AIæ€»ç»“\n\n"
        title += f"**ç”Ÿæˆæ—¶é—´ï¼š** {current_time}\n\n"
        title += f"**AIæ¨¡å‹ï¼š** {api_name}\n\n"
        title += f"**æ€»ä»“åº“æ•°ï¼š** {len(starred)} ä¸ª\n\n"
        title += "---\n\n"
        
        lines = [title]
        
        # æ·»åŠ ç›®å½•
        lines.append("## ğŸ“– ç›®å½•\n\n")
        lang_counts = {}
        for lang, repos in classified.items():
            lang_counts[lang] = len(repos)
        
        for lang, count in sorted(lang_counts.items(), key=lambda x: -x[1]):
            lines.append(f"- [{lang}](#-{lang.lower().replace(' ', '-').replace('+', 'plus').replace('#', 'sharp')})ï¼ˆ{count}ä¸ªï¼‰\n")
        lines.append("\n---\n\n")
        
        printed_repos = set()
        printed_langs = set()  # è®°å½•å·²è¾“å‡ºçš„è¯­è¨€
        
        total_repos = sum(len(repos) for repos in classified.values())
        processed_repos = 0
        
        for lang, repos in sorted(classified.items(), key=lambda x: -len(x[1])):
            if lang in printed_langs:
                continue  # è·³è¿‡å·²è¾“å‡ºçš„è¯­è¨€æ ‡é¢˜
            printed_langs.add(lang)
            print(f"æ­£åœ¨å¤„ç† {lang} ç±»å‹çš„ä»“åº“ï¼ˆå…±{len(repos)}ä¸ªï¼‰...")
            
            # æ·»åŠ è¯­è¨€æ ‡é¢˜å’Œå›¾æ ‡
            lang_icon = {
                "Python": "ğŸ", "JavaScript": "ğŸŸ¨", "TypeScript": "ğŸ”·", 
                "Java": "â˜•", "Go": "ğŸ¹", "Rust": "ğŸ¦€", "C++": "âš¡", 
                "C": "ğŸ”§", "C#": "ğŸ’œ", "PHP": "ğŸ˜", "Ruby": "ğŸ’", 
                "Swift": "ğŸ¦", "Kotlin": "ğŸ…º", "Dart": "ğŸ¯", 
                "Shell": "ğŸš", "HTML": "ğŸŒ", "CSS": "ğŸ¨", 
                "Vue": "ğŸ’š", "React": "âš›ï¸", "Other": "ğŸ“¦"
            }.get(lang, "ğŸ“")
            
            lines.append(f"## {lang_icon} {lang}ï¼ˆå…±{len(repos)}ä¸ªï¼‰\n\n")
            
            for i in range(0, len(repos), BATCH_SIZE):
                this_batch = repos[i:i+BATCH_SIZE]
                print(f"å¤„ç†æ‰¹æ¬¡ {i//BATCH_SIZE + 1}ï¼ŒåŒ…å« {len(this_batch)} ä¸ªä»“åº“...")
                
                # æ ¹æ®é€‰æ‹©ä½¿ç”¨ä¸åŒçš„æ€»ç»“å‡½æ•°
                if use_copilot_api:
                    summaries = copilot_summarize_batch(this_batch, old_summaries)
                else:
                    summaries = summarize_batch(this_batch, old_summaries, use_copilot=False)
                
                for repo, summary in zip(this_batch, summaries):
                    if repo['full_name'] in printed_repos:
                        continue  # è·³è¿‡å·²è¾“å‡ºçš„ä»“åº“
                    printed_repos.add(repo['full_name'])
                    
                    # è·å–ä»“åº“ä¿¡æ¯
                    url = repo["html_url"]
                    stars = repo.get("stargazers_count", 0)
                    forks = repo.get("forks_count", 0)
                    language = repo.get("language", "Unknown")
                    updated_at = repo.get("updated_at", "")
                    if updated_at:
                        try:
                            # è§£ææ—¶é—´å¹¶æ ¼å¼åŒ–
                            from datetime import datetime
                            dt = datetime.fromisoformat(updated_at.replace('Z', '+00:00'))
                            updated_at = dt.strftime("%Y-%m-%d")
                        except:
                            updated_at = updated_at[:10]  # å–å‰10ä¸ªå­—ç¬¦ä½œä¸ºæ—¥æœŸ
                    
                    # æ„å»ºä»“åº“æ¡ç›®
                    lines.append(f"### ğŸ“Œ [{repo['full_name']}]({url})\n\n")
                    
                    # æ·»åŠ ä»“åº“å…ƒä¿¡æ¯
                    lines.append(f"**â­ Stars:** {stars:,} | **ğŸ´ Forks:** {forks:,} | **ğŸ“… æ›´æ–°:** {updated_at}\n\n")
                    
                    # æ·»åŠ AIæ€»ç»“å†…å®¹
                    if summary and summary.strip():
                        lines.append(f"{summary}\n\n")
                    else:
                        lines.append("*æš‚æ— AIæ€»ç»“*\n\n")
                    
                    lines.append("---\n\n")
                    processed_repos += 1
                
                print(f"å·²å¤„ç† {processed_repos}/{total_repos} ä¸ªä»“åº“")
                time.sleep(RATE_LIMIT_DELAY)  # é¿å… API é™æµ
        
        # æ·»åŠ é¡µè„š
        lines.append(f"\n## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯\n\n")
        lines.append(f"- **æ€»ä»“åº“æ•°ï¼š** {processed_repos} ä¸ª\n")
        lines.append(f"- **ç¼–ç¨‹è¯­è¨€æ•°ï¼š** {len(classified)} ç§\n")
        lines.append(f"- **ç”Ÿæˆæ—¶é—´ï¼š** {current_time}\n")
        lines.append(f"- **AIæ¨¡å‹ï¼š** {api_name}\n\n")
        lines.append("---\n\n")
        lines.append("*æœ¬æ–‡æ¡£ç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œå¦‚æœ‰é”™è¯¯è¯·ä»¥åŸä»“åº“ä¿¡æ¯ä¸ºå‡†ã€‚*\n")
        
        # å†™å…¥æ–‡ä»¶
        with open(README_SUM_PATH, "w", encoding="utf-8") as f:
            f.write(''.join(lines))
        
        print(f"\nâœ… {README_SUM_PATH} å·²ç”Ÿæˆï¼Œå…±å¤„ç†äº† {processed_repos} ä¸ªä»“åº“ã€‚")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
